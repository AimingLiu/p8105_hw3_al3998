---
title: "p8105_hw3_al3998"
author: "AimingLiu"
date: "10/3/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(dplyr)
```
# Problem 1
## Load the data from the p8105.datasets
```{r}
library(p8105.datasets)
data("instacart")
```

```{r}
  aisle_form = instacart %>% 
  group_by(aisle) %>%
  summarize(n = n()) %>% 
  arrange(desc(n)) 
  aisle_num = nrow(aisle_form) 
```
 It can be concluded that the row number of instacart is  `r nrow(instacart)` and  the column number of instacart is `r ncol(instacart)`.
 The key variables in this dataset include `r instacart %>% select(user_id:product_name) %>% colnames` and `r instacart %>% select(aisle) %>% colnames`.And for example,when we see user whose id is `r pull(instacart,user_id)[1]` we can know when he/she bought it and the information about the products.
 It can be concluded that the row number of aisle is  `r nrow(aisle_form)` and  the most items ordered from `r pull(aisle_form,aisle)[1]`
 
## Making a plot
```{r  fig.height = 10, fig.width = 15}
  aisle_form %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle,y = n,fill = aisle))+
    geom_col()+coord_flip()+
  labs(
    title = "Plot about the number of items ordered in each aisle",
    x = "number of items",
    y = "aisle" ,
    caption = "Data from the instacart")+
    theme(legend.position = "bottom")
```
There are  `r aisle_form %>%filter(n>10000) %>% pull(n) %>% length()` aisle have items ordered more than 10000.The aisle which was ordered most is `r pull(aisle_form,aisle)[1]` and the number of it is `r pull(aisle_form,n)[1]`.The aisle which was ordered least is `r pull(aisle_form,aisle)[134]` and the number of it is `r pull(aisle_form,n)[134]`.

## Making a table showing the three most popular items 

```{r}
 pop_form = instacart %>% 
 filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
 group_by(aisle,product_name) %>% 
 summarise(n_aisle_product = n()) %>%
 group_by(aisle) %>%
 filter(min_rank(desc(n_aisle_product))<4) 
 
  knitr::kable(pop_form)
 
```

The table is a `r nrow(pop_form)`* `r ncol(pop_form)` table.The three most popular items in baking ingredients are `r pop_form%>%filter(aisle=="baking ingredients")%>% pull(product_name)`,the three most popular items in dog food care are  `r pop_form%>%filter(aisle=="dog food care")%>% pull(product_name)`,the three most popular items in packaged vegetables fruits are `r pop_form%>%filter(aisle=="packaged vegetables fruits")%>% pull(product_name)`.And as we can see,the number of items in packaged vegetables fruits are much higher than dog food care which means that packaged vegetables fruits is the most popular one in these three aisle. 


## Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
  mean_hour_form = instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_order = mean(order_hour_of_day)) %>% 
  mutate(order_dow = recode(order_dow,"0"="Sunday","1"="Monday","2"="Tuesday","3"="Wednesday","4"="Thursday","5"="Friday","6"="Saturday")) %>% 
 pivot_wider(names_from = order_dow,
             values_from = mean_order) 
  
 knitr::kable(mean_hour_form)
 
```

This is  a `r nrow(mean_hour_form)`* `r ncol(mean_hour_form)` table.As we can see from the result,on each day of a week,the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered are more than 11 and less than 16.And also on each day of a week,the mean hour of the day in Coffee Ice Cream is larger than Pink Lady Apples except Friday.

# Problem 2
```{r}
  library(p8105.datasets)
  data("brfss_smart2010")
```


```{r}
  brfss_form = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic =="Overall Health") %>% 
  drop_na(response) %>% 
  mutate(response = factor(response,
                           levels = c("Poor","Fair","Good","Very good","Excellent"),
                           ordered = is.ordered(response))) %>% 
  arrange(desc(response))  
```

```{r}
  loco_2002 = filter(brfss_form ,year == "2002") %>% 
  group_by(locationabbr) %>% 
  summarise(count_state =n()) %>% 
  filter(count_state >= 7) 
```

In 2002,the abbreviation of the states which were observed at 7 or more locations were `r pull(loco_2002,locationabbr)`

```{r}
  loco_2010 = filter(brfss_form ,year == "2010") %>% 
  group_by(locationabbr) %>% 
  summarise(count_state =n()) %>% 
  filter(count_state >= 7) 
```
In 2010,the abbreviation of the states which were observed at 7 or more locations were `r pull(loco_2010,locationabbr)`

```{r}
  excellent_form = brfss_form %>% 
  filter(response == "Excellent") %>% 
  group_by(locationabbr,year) %>% 
  summarize(mean_data = mean(data_value,na.rm = TRUE)) 

```

```{r}
  average_plot = excellent_form %>% 
  ggplot(aes(x = year,y = mean_data,color = locationabbr ))+
  geom_line()+
  labs(
    title = "Spaghetti Plot",
    x = "Year",
    y = "average of the data value"
  ) 
  
average_plot
```
The name of x-axis is `Year`,and the name of y-axis is `average of the data value`.The smallest value is `r excellent_form %>%ungroup(locationabbr) %>% filter(mean_data == min(mean_data))%>% pull(mean_data)` which is in `r excellent_form %>%ungroup(locationabbr) %>% filter(mean_data == min(mean_data))%>% pull(locationabbr)` in year `2005` and the largest value is `r excellent_form%>%ungroup(locationabbr) %>% filter(mean_data == max(mean_data))%>% pull(mean_data)` which is in `r excellent_form %>% ungroup(locationabbr) %>% filter(mean_data == max(mean_data))%>% pull(locationabbr)` in year `2002`

## Make a two-panel plot
```{r fig.height = 5, fig.width = 10}
 brfss_form %>% 
 filter(locationabbr == "NY" , year %in% c ("2006","2010")) %>% 
 select(locationdesc,response,data_value,year) %>% 
 ggplot(aes(x = response ,y = data_value) )+
 geom_line(aes(color = locationdesc, group = locationdesc)) + 
  labs(
     title = " distribution of data_value for responses in 2006 and 2010",
     x = "response",
     y = "data_value"
  )+ theme(legend.position = "bottom", plot.title = element_text(size = 24)) +
    facet_grid(~year)  
```
As we can see from the the two-panel plot,both in year `2006` and `2010`,the data value of `Poor` for all the locationdesc is the lowest among different responses.In year `2006` ,in Kings county and Queens county,the data value for `Good` is the highest among different responses,and also the changing curve are similar to each other in these two locationdescs.In other locationdescs, the data value for `Very good` is the highest among different responses.In year `2010`,in Kings county,Queens county and Bronx county,the data value for `Good` is the highest among different responses,in other locationdescs, the data value for `Very good` is the highest among different responses.


```{r}
  ny_2006 = brfss_form %>% 
  filter( locationabbr=="NY") %>% 
  select(year,locationdesc,data_value,response) %>% 
  filter(year == "2006") %>% 
  group_by(response) %>% 
  mutate(mean_NY = mean(data_value,na.rm=TRUE)) %>% 
  ggplot(aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4)+
  xlim(-5,50) +
  labs(
       x    = "data_value",
       title = " Distribution of NY State in 2006"
       )

  ny_2010 = brfss_form %>% 
  filter( locationabbr=="NY") %>% 
  select(year,locationdesc,data_value,response) %>% 
  filter(year == "2010") %>% 
  group_by(response) %>% 
  mutate(mean_NY = mean(data_value,na.rm=TRUE)) %>% 
  ggplot(aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4)+
  xlim(-5,50) +
  labs(
       x    = "data_value",
       title = " Distribution of NY State in 2010"
       )
  
 ny_2006/ny_2010
```


As we can see from the two-panel plot,in year 2006,the data_value for response `Poor` is lowest and is very concenreated.The degree of concentration of fair is more dispersive than `Poor` but is more concentrated than `Excellent`,`Good` and `Very good`.The degrees of concentration in  `Excellent` and `Very good`  are similar to each other.The data_value of ` Good` is the most dispersive one.

In year 2010,the data_value for response `Poor` is also the lowest one and is very concenreated.The degrees of concentration in  `Fair`,`Very good` and `Good` are similar to each other.The data_value of `Very good` is the most dispersive one and it disperses from around 20 to 50.

Compared the two graphics, the location of `Poor`,`Excellent` and `Very good` are similar.The curve of `Fair` is more dispersive in 2010 than in 2006 and the curve of `Good` is more dispersive in 2006.

# Problem 3
```{r message=F}
  accel_data = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(cols = 4:1443,names_to = "activity",names_prefix = "activity_",values_to = "activity_count") %>% 
    mutate(activity = factor(activity,level = c(1:1440)),weekday_end = case_when(day =="Monday" ~ "weekday",
                                    day =="Tuesday" ~ "weekday",
                                    day =="Wednesday" ~ "weekday",
                                    day =="Thursday" ~ "weekday",
                                     day =="Friday" ~ "weekday",
                                    day =="Saturday" ~ "weekend",
                                    day =="Sunday" ~ "weekend"
                                )) %>% 
  arrange(week,day_id) %>%
  select(1:3,weekday_end,everything()) 
  
```
For the resulting dataset,there are `r nrow(accel_data)` observations and `r ncol(accel_data)` variables.The data set also include the information about week,day id,weekday and weekend.

## create a table showing the totals
```{r}
         total_accel_data = accel_data %>% 
         group_by(week,day) %>% 
         mutate(accel_sum = sum(activity_count)) %>%
         select(week:weekday_end, accel_sum) %>%  
         distinct()  
         
  knitr::kable(total_accel_data,digits = 0)
  
```
```{r}
  total_accel_data %>% 
  ungroup(week) %>% 
  ggplot(aes(x = day,y = accel_sum,color = week))+
  geom_line()+
  labs(
    title = "Plot Sum 1",
    x = "day",
    y = "accel_sum"
  )
```
It is hard for us to conclude a trend from the table,so I draw a plot to help me see the trends.As we can see from the `Plot Sum 1`,the range of `accel_sum` in Tuesday is the smallest and the range of `accel_sum` in Monday and Saturday are similar to each other which are much larger than the smallest one. 


```{r}
 total_accel_data %>% 
  ungroup(week,day_id) %>% 
  ggplot(aes(x = day_id,y = accel_sum,color = week))+
  geom_line()+
  labs(
    title = "Plot Sum 2 ",
    x = "day_id",
    y = "accel_sum"
  )
```

As we can see from the `Plot Sum 2`.when the  `day_id` is `r total_accel_data %>% ungroup(day_id) %>% filter(accel_sum == min(accel_sum))%>% pull(day_id)`,the sum of activity count is lowest,which is `r total_accel_data %>% ungroup(day_id) %>% filter(accel_sum == min(accel_sum))%>%distinct(accel_sum)%>%pull(accel_sum)`.and when the `day_id` is `r total_accel_data %>% ungroup(day_id) %>% filter(accel_sum == max(accel_sum))%>% pull(day_id)`,the sum of activity count is largest,which is `r total_accel_data %>% ungroup(day_id) %>% filter(accel_sum == max(accel_sum))%>%distinct(accel_sum)%>%pull(accel_sum)`.
From week one to week two,there is a trend in increasing.In week three,the fluctuation is to some extent weak and  then in week four and five,the fluctuations are very dramatic.


## Make a single-panel plot that shows the 24-hour activity time courses
```{r}
  weekdays <-c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
  accel_data %>% 
  mutate(day = factor(day,levels = weekdays,ordered = T)) %>% 
  ggplot(aes(x = activity,y = activity_count,color = day))+
  geom_line(alpha = .2)+
  labs(
    title = "24-hour activity time courses ",
    x = "activity",
    y = "activity_count"
  )
```
  
  
  As we can see from the graphic,the `activity_count` of the 24-hour activity time courses is very low at the beginning of this graphic in almost everyday in a week which means that during this period of time the counts of activity is small,and this peroid of time is around the  midnight and can make sense.Also, we can see from the graphic,there are about 2~3 peaks in `activity_count` which means during these peroid of time in  everyday,the counts of activity is larger than counts during other period of time in a day,and the time of peaks maybe around 7:00 in the morning,12:00 at noon and 19:00 to 20:00 at night.








